{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural_Machine_Translation_Model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B0QEggYAbaKt","outputId":"699472c5-8970-4e1d-f73a-fd79d88564a0"},"source":["!wget -O deu.txt https://www.dropbox.com/s/unjqe9n6pr74ekw/deu.txt?dl=0"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-11-23 11:28:40--  https://www.dropbox.com/s/unjqe9n6pr74ekw/deu.txt?dl=0\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.7.18, 2620:100:6019:18::a27d:412\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.7.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/unjqe9n6pr74ekw/deu.txt [following]\n","--2021-11-23 11:28:41--  https://www.dropbox.com/s/raw/unjqe9n6pr74ekw/deu.txt\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc8e636240b17428602790ea9c4c.dl.dropboxusercontent.com/cd/0/inline/BaicKYxZztq4sLKXXVWa8AKQ5a3GTi5579inuxbOwdTm9J4QwM7iRk_x_avISUpNm_EViXBfdpZsWZYwEmpcjMxST3oZuI8rFJYv5KGbiIX4O8bkPMandiOn3qQ7aa3-LFfuMg2W_dF_ysPUScZGGEEp/file# [following]\n","--2021-11-23 11:28:41--  https://uc8e636240b17428602790ea9c4c.dl.dropboxusercontent.com/cd/0/inline/BaicKYxZztq4sLKXXVWa8AKQ5a3GTi5579inuxbOwdTm9J4QwM7iRk_x_avISUpNm_EViXBfdpZsWZYwEmpcjMxST3oZuI8rFJYv5KGbiIX4O8bkPMandiOn3qQ7aa3-LFfuMg2W_dF_ysPUScZGGEEp/file\n","Resolving uc8e636240b17428602790ea9c4c.dl.dropboxusercontent.com (uc8e636240b17428602790ea9c4c.dl.dropboxusercontent.com)... 162.125.4.15, 2620:100:6019:15::a27d:40f\n","Connecting to uc8e636240b17428602790ea9c4c.dl.dropboxusercontent.com (uc8e636240b17428602790ea9c4c.dl.dropboxusercontent.com)|162.125.4.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11303014 (11M) [text/plain]\n","Saving to: ‘deu.txt’\n","\n","deu.txt             100%[===================>]  10.78M  --.-KB/s    in 0.1s    \n","\n","2021-11-23 11:28:41 (103 MB/s) - ‘deu.txt’ saved [11303014/11303014]\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ru9h8w9Yfddc","outputId":"e33005f6-ef8a-4fd1-fea4-112fef5e7fd2"},"source":["import string\n","import re\n","from pickle import dump\n","from unicodedata import normalize\n","from numpy import array\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, mode='rt', encoding='utf-8')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n"," \n","# split a loaded document into sentences\n","def to_sentences(doc):\n","\treturn doc.strip().split('\\n')\n"," \n","# shortest and longest sentence lengths\n","def sentence_lengths(sentences):\n","\tlengths = [len(s.split()) for s in sentences]\n","\treturn min(lengths), max(lengths)\n"," \n","# load English data\n","filename = 'europarl-v7.fr-en.en'\n","doc = load_doc(filename)\n","sentences = to_sentences(doc)\n","minlen, maxlen = sentence_lengths(sentences)\n","print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n"," \n","# load French data\n","filename = 'europarl-v7.fr-en.fr'\n","doc = load_doc(filename)\n","sentences = to_sentences(doc)\n","minlen, maxlen = sentence_lengths(sentences)\n","print('French data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved: english-german.pkl\n"]}]},{"cell_type":"code","metadata":{"id":"89KCAcuqcQlw"},"source":["from pickle import load\n","from pickle import dump\n","from numpy.random import shuffle\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, mode='rt', encoding='utf-8')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n"," \n","# split a loaded document into sentences\n","def to_sentences(doc):\n","\treturn doc.strip().split('\\n')\n"," \n","# clean a list of lines\n","def clean_lines(lines):\n","\tcleaned = list()\n","\t# prepare regex for char filtering\n","\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n","\t# prepare translation table for removing punctuation\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor line in lines:\n","\t\t# normalize unicode characters\n","\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n","\t\tline = line.decode('UTF-8')\n","\t\t# tokenize on white space\n","\t\tline = line.split()\n","\t\t# convert to lower case\n","\t\tline = [word.lower() for word in line]\n","\t\t# remove punctuation from each token\n","\t\tline = [word.translate(table) for word in line]\n","\t\t# remove non-printable chars form each token\n","\t\tline = [re_print.sub('', w) for w in line]\n","\t\t# remove tokens with numbers in them\n","\t\tline = [word for word in line if word.isalpha()]\n","\t\t# store as string\n","\t\tcleaned.append(' '.join(line))\n","\treturn cleaned\n"," \n","# save a list of clean sentences to file\n","def save_clean_sentences(sentences, filename):\n","\tdump(sentences, open(filename, 'wb'))\n","\tprint('Saved: %s' % filename)\n"," \n","# load English data\n","filename = 'europarl-v7.fr-en.en'\n","doc = load_doc(filename)\n","sentences = to_sentences(doc)\n","sentences = clean_lines(sentences)\n","save_clean_sentences(sentences, 'english.pkl')\n","# spot check\n","for i in range(10):\n","\tprint(sentences[i])\n"," \n","# load French data\n","filename = 'europarl-v7.fr-en.fr'\n","doc = load_doc(filename)\n","sentences = to_sentences(doc)\n","sentences = clean_lines(sentences)\n","save_clean_sentences(sentences, 'french.pkl')\n","# spot check\n","for i in range(10):\n","\tprint(sentences[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7lodBdUXcUL_"},"source":["from pickle import load\n","from pickle import dump\n","from collections import Counter\n"," \n","# load a clean dataset\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n"," \n","# save a list of clean sentences to file\n","def save_clean_sentences(sentences, filename):\n","\tdump(sentences, open(filename, 'wb'))\n","\tprint('Saved: %s' % filename)\n"," \n","# create a frequency table for all words\n","def to_vocab(lines):\n","\tvocab = Counter()\n","\tfor line in lines:\n","\t\ttokens = line.split()\n","\t\tvocab.update(tokens)\n","\treturn vocab\n"," \n","# remove all words with a frequency below a threshold\n","def trim_vocab(vocab, min_occurance):\n","\ttokens = [k for k,c in vocab.items() if c >= min_occurance]\n","\treturn set(tokens)\n"," \n","# mark all OOV with \"unk\" for all lines\n","def update_dataset(lines, vocab):\n","\tnew_lines = list()\n","\tfor line in lines:\n","\t\tnew_tokens = list()\n","\t\tfor token in line.split():\n","\t\t\tif token in vocab:\n","\t\t\t\tnew_tokens.append(token)\n","\t\t\telse:\n","\t\t\t\tnew_tokens.append('unk')\n","\t\tnew_line = ' '.join(new_tokens)\n","\t\tnew_lines.append(new_line)\n","\treturn new_lines\n"," \n","# load English dataset\n","filename = 'english.pkl'\n","lines = load_clean_sentences(filename)\n","# calculate vocabulary\n","vocab = to_vocab(lines)\n","print('English Vocabulary: %d' % len(vocab))\n","# reduce vocabulary\n","vocab = trim_vocab(vocab, 5)\n","print('New English Vocabulary: %d' % len(vocab))\n","# mark out of vocabulary words\n","lines = update_dataset(lines, vocab)\n","# save updated dataset\n","filename = 'english_vocab.pkl'\n","save_clean_sentences(lines, filename)\n","# spot check\n","for i in range(10):\n","\tprint(lines[i])\n"," \n","# load French dataset\n","filename = 'french.pkl'\n","lines = load_clean_sentences(filename)\n","# calculate vocabulary\n","vocab = to_vocab(lines)\n","print('French Vocabulary: %d' % len(vocab))\n","# reduce vocabulary\n","vocab = trim_vocab(vocab, 5)\n","print('New French Vocabulary: %d' % len(vocab))\n","# mark out of vocabulary words\n","lines = update_dataset(lines, vocab)\n","# save updated dataset\n","filename = 'french_vocab.pkl'\n","save_clean_sentences(lines, filename)\n","# spot check\n","for i in range(10):\n","\tprint(lines[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ruQyoWzYyOy6"},"source":["from numpy import array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.utils.vis_utils import plot_model\n","from keras.models import Sequential\n","from keras.layers import LSTM\n","from keras.layers import Dense\n","from keras.layers import Embedding\n","from keras.layers import RepeatVector\n","from keras.layers import TimeDistributed\n","from keras.callbacks import ModelCheckpoint\n"," \n","# load a clean dataset\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n"," \n","# fit a tokenizer\n","def create_tokenizer(lines):\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n"," \n","# max sentence length\n","def max_length(lines):\n","\treturn max(len(line.split()) for line in lines)\n"," \n","# encode and pad sequences\n","def encode_sequences(tokenizer, length, lines):\n","\t# integer encode sequences\n","\tX = tokenizer.texts_to_sequences(lines)\n","\t# pad sequences with 0 values\n","\tX = pad_sequences(X, maxlen=length, padding='post')\n","\treturn X\n"," \n","# one hot encode target sequence\n","def encode_output(sequences, vocab_size):\n","\tylist = list()\n","\tfor sequence in sequences:\n","\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n","\t\tylist.append(encoded)\n","\ty = array(ylist)\n","\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n","\treturn y\n"," \n","# define NMT model\n","def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n","\tmodel = Sequential()\n","\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n","\tmodel.add(LSTM(n_units))\n","\tmodel.add(RepeatVector(tar_timesteps))\n","\tmodel.add(LSTM(n_units, return_sequences=True))\n","\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n","\treturn model\n"," \n","# load datasets\n","dataset = load_clean_sentences('french.pkl')\n","train = load_clean_sentences('french.pkl')\n","test = load_clean_sentences('french.pkl')\n"," \n","# prepare english tokenizer\n","eng_tokenizer = create_tokenizer(dataset[:, 0])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length(dataset[:, 0])\n","print('English Vocabulary Size: %d' % eng_vocab_size)\n","print('English Max Length: %d' % (eng_length))\n","# prepare german tokenizer\n","ger_tokenizer = create_tokenizer(dataset[:, 1])\n","ger_vocab_size = len(ger_tokenizer.word_index) + 1\n","ger_length = max_length(dataset[:, 1])\n","print('French Vocabulary Size: %d' % ger_vocab_size)\n","print('French Max Length: %d' % (ger_length))\n"," \n","# prepare training data\n","trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n","trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n","trainY = encode_output(trainY, eng_vocab_size)\n","# prepare validation data\n","testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n","testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n","testY = encode_output(testY, eng_vocab_size)\n"," \n","# define model\n","model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n","model.compile(optimizer='adam', loss='categorical_crossentropy')\n","# summarize defined model\n","print(model.summary())\n","plot_model(model, to_file='model.png', show_shapes=True)\n","# fit model\n","filename = 'model.h5'\n","checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y2qwtzeayVeY"},"source":["from pickle import load\n","from numpy import array\n","from numpy import argmax\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import load_model\n","from nltk.translate.bleu_score import corpus_bleu\n"," \n","# load a clean dataset\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n"," \n","# fit a tokenizer\n","def create_tokenizer(lines):\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n"," \n","# max sentence length\n","def max_length(lines):\n","\treturn max(len(line.split()) for line in lines)\n"," \n","# encode and pad sequences\n","def encode_sequences(tokenizer, length, lines):\n","\t# integer encode sequences\n","\tX = tokenizer.texts_to_sequences(lines)\n","\t# pad sequences with 0 values\n","\tX = pad_sequences(X, maxlen=length, padding='post')\n","\treturn X\n"," \n","# map an integer to a word\n","def word_for_id(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None\n"," \n","# generate target given source sequence\n","def predict_sequence(model, tokenizer, source):\n","\tprediction = model.predict(source, verbose=0)[0]\n","\tintegers = [argmax(vector) for vector in prediction]\n","\ttarget = list()\n","\tfor i in integers:\n","\t\tword = word_for_id(i, tokenizer)\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\ttarget.append(word)\n","\treturn ' '.join(target)\n"," \n","# evaluate the skill of the model\n","def evaluate_model(model, tokenizer, sources, raw_dataset):\n","\tactual, predicted = list(), list()\n","\tfor i, source in enumerate(sources):\n","\t\t# translate encoded source text\n","\t\tsource = source.reshape((1, source.shape[0]))\n","\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n","\t\traw_target, raw_src = raw_dataset[i]\n","\t\tif i < 10:\n","\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n","\t\tactual.append([raw_target.split()])\n","\t\tpredicted.append(translation.split())\n","\t# calculate BLEU score\n","\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n","\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n"," \n","# load datasets\n","dataset = load_clean_sentences('french.pkl')\n","train = load_clean_sentences('french.pkl')\n","test = load_clean_sentences('french.pkl')\n","# prepare english tokenizer\n","eng_tokenizer = create_tokenizer(dataset[:, 0])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length(dataset[:, 0])\n","# prepare german tokenizer\n","ger_tokenizer = create_tokenizer(dataset[:, 1])\n","ger_vocab_size = len(ger_tokenizer.word_index) + 1\n","ger_length = max_length(dataset[:, 1])\n","# prepare data\n","trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n","testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n"," \n","# load model\n","model = load_model('model.h5')\n","# test on some training sequences\n","print('train')\n","evaluate_model(model, eng_tokenizer, trainX, train)\n","# test on some test sequences\n","print('test')\n","evaluate_model(model, eng_tokenizer, testX, test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QNuOQ15bg4Fe"},"source":["Reference: \n","\n","Deep Learning for Natural Language Processing by Jaon Brownlee\n","\n","https://machinelearningmastery.com/deep-learning-for-nlp/"]}]}